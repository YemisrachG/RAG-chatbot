{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f431f66b-8d94-4a03-b37e-f33cdf5b8bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[32m     12\u001b[39m DATA_DIR = \u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.chains'"
     ]
    }
   ],
   "source": [
    "# rag_pipeline.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.chains import LLMChain\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = '../data'\n",
    "VECTOR_STORE_DIR = '../vector_store/chroma_db_credi_trust'\n",
    "FILTERED_DATA_FILE = os.path.join(DATA_DIR, 'filtered_complaints.csv')\n",
    "NARRATIVE_COLUMN = 'Consumer complaint narrative'\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(FILTERED_DATA_FILE)\n",
    "documents = [Document(page_content=text) for text in df[NARRATIVE_COLUMN].dropna().tolist()]\n",
    "\n",
    "# --- Embedding Setup ---\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# --- Vector Store Setup ---\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=VECTOR_STORE_DIR\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "# --- Retriever Function ---\n",
    "def retrieve_chunks(question, k=5):\n",
    "    return vectorstore.similarity_search(question, k=k)\n",
    "\n",
    "# --- Prompt Template ---\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
    "Use the following retrieved complaint excerpts to formulate your answer.\n",
    "\n",
    "If the context doesn't contain the answer, state that you don't have enough information.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- Language Model (LLM) Setup ---\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-large\",  # You can switch to mistralai/Mistral-7B-Instruct or another HF model\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_length\": 512}\n",
    ")\n",
    "\n",
    "rag_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "# --- RAG Pipeline Function ---\n",
    "def answer_question(question: str, k: int = 5) -> dict:\n",
    "    retrieved_docs = retrieve_chunks(question, k=k)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    response = rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\n",
    "        \"answer\": response[\"text\"],\n",
    "        \"sources\": [doc.page_content for doc in retrieved_docs]\n",
    "    }\n",
    "\n",
    "# --- Test Run ---\n",
    "if __name__ == \"__main__\":\n",
    "    test_question = \"How do customers feel about credit card disputes?\"\n",
    "    result = answer_question(test_question)\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(result['answer'])\n",
    "    print(\"\\n--- Retrieved Sources ---\")\n",
    "    for i, src in enumerate(result['sources']):\n",
    "        print(f\"\\nSource {i+1}:\\n{src}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c63d26-ab7d-490a-ba7d-2d93a533e748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656964c-36d2-4fdd-b3ed-b2dcf1f4f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
